# Class 17 - Web Scraping

## [Web Scrape with Python in 4 Minutes](https://towardsdatascience.com/how-to-web-scrape-with-python-in-4-minutes-bc49186a8460)

Important notes about web scraping:

* Read through the websites terms and conditions to understand how you can legally use the data. Most commericial use is prohibited.
* Make sure you are not downloading data too fast, you may break the website and get blocked.

*Import libraries*:

```Python
import requests
import urllib.request
import time
from bs4 import BeautifulSoup
```

*Set URL to website and access site with our requests library*:

```Python
url = 'http://web.mta.info/developers/turnstile.html'
response = requests.get(url)
```

*If the access was successful you should see the following output*:

```Python
In [9]: response #200 means it went through
Out[9]: <Response [200]>
```

*Next we parse the html with BeautifulSoup so we can work with a nicer data structure*:

```Python
soup = BeautifulSoup(response.txt, "html.parser")
```

*We use the method `.findAll` to locate all of our `<a>` tags*:

```Python
soup.findAll('a')
```

*Extract the actual link we want, test out the first link*:

```Python
one_a_tag = soup.findAll('a')[38]
link = one_a_tag['href']
```

*Use our urllib.request library to download file path to our computer. Provide request.urlretrieve with two parameters: File URL and filename*:

```Python
donwload_url = 'http://web.mta.info/developers/'+ link
urllib.request.urlretrieve(download_url,'./'+link[link.find('/tunrstile_')+1:])
```

*Include this line of code so we can pause our code for a second so we're not spamming the website with requests (gets you flagged as a spammer)*:

```Python
time.sleep(1)
```

*Dowloading the entire set of data files with a for lop*:

```Python
# Import libraries
import requests
import urllib.request
import time
from bs4 import BeautifulSoup

# Set the URL you want to webscrape from
url = 'http://web.mta.info/developers/turnstile.html'

# Connect to the URL

response = requests.get(url)

# Parse HTML and save to BeautifulSoup object
soup = BeautifulSoup(response.text, "html.parser")

# To download the whole data set, let's do a for loop through all a tags
line_count = 1 #variable to track what line you're on
for one_a_tag in soup.findAll('a'): #'a' tags are for links
  if line_count >=36: #code for text files starting at line 36
      link = one_a_tag['href']
      download_url = 'http://web.mta.info/developers/'+ link
      urllib.request.urlretrieve(download_url, './'+link[link.find('/turnstile_')+1:])
      time.sleep(1) #pause the code for a second
    # add 1 for next line
    line_count += 1

```

---

## [What is Web Scraping?](https://en.wikipedia.org/wiki/Web_scraping)

* Also known as web harvesting or web data extraction

* Can be done manually by a software user, but the term typically refers to automated processes implemented using a bot or web crawler.

* Form of coyping in which specific data is gathered and copied from the web - typically into a central local database or spreadhseet.

---

## [How to Scrape Websites Without Getting Blocked](https://www.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/)

* Web scraping is a task that has to be performed responsibly so that it does not have a detrimental effect on the sites being scraped.

* An underpowered server would have a hard time keeping up with too many requests from too many crawlers.

* **Best practices**:
  * Respect Robots.txt
  * Make the crawling slower, do not slam the server, treat websites nicely
  * Do not follow the same crawling pattern
  * Make requests through Proxies and rotate them as needed
  * Rotate User Agents and corresponding HTTP Request Headers between requests
  * Use a headless browser like Puppeteer, Selenium or Playwright
  * Beware of Honey Pot Traps
  * Check if Website is changing layouts
  * Avoid scraping data behind a login

---

## [Track Amazon Prices](https://www.youtube.com/watch?v=Bg9r_yLk7VY)

---

## [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/)
